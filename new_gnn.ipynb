{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 19818], type=[4331], canonical_smiles=[4331], MolWt=[4331], LogP=[4331], NumHDonors=[4331], NumHAcceptors=[4331], NumRotatableBonds=[4331], NumAromaticRings=[4331], assay_description=[4331], name=[4331], description=[4331], proteinDescription=[4331], molecule_chembl_id=[4331], chromosome=[4331], summary=[4331], aliases=[4331], compound_effectiveness=[4331], entrez_id=[4331], target_chembl_id=[4331], keywords=[4331], gene_type=[4331], proteinExistence=[4331], gpl_id=[4331], target_organism=[4331], sequence=[4331], platform_title=[4331], protein_names=[4331], standard_type=[4331], uniProtkbId=[4331], organism=[4331], uniprot_id=[4331], taxon=[4331], telomerase_activity=[4331], standard_units=[4331], annotationScore=[4331], standard_value=[4331], entryType=[4331], gsm_id=[4331], genes=[4331], pchembl_value=[4331], series_title=[4331], title=[4331], activity_type=[19818], activity_value=[19818], activity_units=[19818], edge_assay_description=[19818], similarity=[19818], relationship=[19818], relation=[19818], num_nodes=4331)\n",
      "Number of nodes: 4331\n",
      "Number of edges: 19818\n",
      "Node features: 0\n",
      "Edge features: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Load the knowledge graph from pkl\n",
    "with open(r\"C:\\Users\\wes\\telomerase_comprehensive_knowledge_graph_updated_5.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# Ensure G is a NetworkX graph\n",
    "if not isinstance(G, nx.Graph):\n",
    "    raise TypeError(\"The loaded object is not a NetworkX graph\")\n",
    "\n",
    "# Function to get all unique attributes\n",
    "def get_all_attrs(items):\n",
    "    all_attrs = set()\n",
    "    for item in items:\n",
    "        if isinstance(item, tuple) and len(item) == 3:  # Edge data\n",
    "            _, _, attrs = item\n",
    "        elif isinstance(item, tuple) and len(item) == 2:  # Node data\n",
    "            _, attrs = item\n",
    "        else:\n",
    "            continue\n",
    "        if isinstance(attrs, dict):\n",
    "            all_attrs.update(attrs.keys())\n",
    "    return list(all_attrs)\n",
    "\n",
    "# Function to create a default value for missing attributes\n",
    "def default_value(attr):\n",
    "    if attr == 'name':\n",
    "        return ''\n",
    "    else:\n",
    "        return 0  # You might want to adjust this based on your data\n",
    "\n",
    "# Normalize node attributes\n",
    "all_node_attrs = get_all_attrs(G.nodes(data=True))\n",
    "for node in G.nodes():\n",
    "    for attr in all_node_attrs:\n",
    "        if attr not in G.nodes[node]:\n",
    "            G.nodes[node][attr] = default_value(attr)\n",
    "\n",
    "# Normalize edge attributes\n",
    "all_edge_attrs = get_all_attrs(G.edges(data=True))\n",
    "for edge in G.edges():\n",
    "    for attr in all_edge_attrs:\n",
    "        if attr not in G.edges[edge]:\n",
    "            G.edges[edge][attr] = default_value(attr)\n",
    "\n",
    "# Convert to PyTorch Geometric format\n",
    "data = from_networkx(G)\n",
    "\n",
    "print(data)  # Check the loaded graph data\n",
    "\n",
    "# Print some additional information about the graph\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Node features: {data.num_node_features}\")\n",
    "print(f\"Edge features: {data.num_edge_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed node features shape: torch.Size([4331, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wes\\Miniconda3\\envs\\gnn\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     91\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m---> 92\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     93\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Example for processing node features\n",
    "node_features = []\n",
    "le = LabelEncoder()\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Convert 'type' to numerical and one-hot encode\n",
    "type_encoded = le.fit_transform(data.type)\n",
    "type_one_hot = torch.eye(len(le.classes_))[type_encoded]\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_features = torch.stack([\n",
    "    torch.tensor(sc.fit_transform(data.MolWt.view(-1, 1)).flatten()),\n",
    "    torch.tensor(sc.fit_transform(data.LogP.view(-1, 1)).flatten()),\n",
    "    torch.tensor(sc.fit_transform(data.NumHDonors.view(-1, 1)).flatten()),\n",
    "    torch.tensor(sc.fit_transform(data.NumHAcceptors.view(-1, 1)).flatten()),\n",
    "    torch.tensor(sc.fit_transform(data.NumRotatableBonds.view(-1, 1)).flatten()),\n",
    "    torch.tensor(sc.fit_transform(data.NumAromaticRings.view(-1, 1)).flatten()),\n",
    "], dim=1)\n",
    "\n",
    "# Combine features\n",
    "node_features = torch.cat([type_one_hot, numerical_features], dim=1)\n",
    "node_features = node_features.float()  # Convert to float32\n",
    "data.x = node_features\n",
    "print(f\"Processed node features shape: {data.x.shape}\")\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "num_nodes = data.num_nodes\n",
    "train_mask, val_mask = train_test_split(range(num_nodes), test_size=0.2, random_state=42)\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_mask] = True\n",
    "data.val_mask[val_mask] = True\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.float()  # Ensure input is float32\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = GCN(num_features=data.num_node_features, hidden_channels=64, num_classes=num_classes)\n",
    "model = model.float()  # Ensure model parameters are float32\n",
    "# Assuming a binary classification task, adjust as needed\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Create a directory for saving models\n",
    "os.makedirs('model_checkpoints', exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1000):  # Increased max epochs\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        val_acc = (out[data.val_mask].argmax(dim=1) == data.y[data.val_mask]).float().mean()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc.item():.4f}')\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save model periodically (every 50 epochs)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        torch.save(model.state_dict(), f'model_checkpoints/model_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'model_checkpoints/best_model.pt')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if counter >= patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.save('model_checkpoints/best_model.pt'))\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    final_val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "    final_val_acc = (out[data.val_mask].argmax(dim=1) == data.y[data.val_mask]).float().mean()\n",
    "\n",
    "print(f'Final Validation Loss: {final_val_loss.item():.4f}, Final Validation Accuracy: {final_val_acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 17:45:13,671 - INFO - Using device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not all nodes contain the same attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 189\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m G \u001b[38;5;241m=\u001b[39m load_knowledge_graph(graph_path)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    191\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData moved to device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 77\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(G):\n\u001b[1;32m---> 77\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Create node features\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     node_features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\wes\\Miniconda3\\envs\\gnn\\Lib\\site-packages\\torch_geometric\\utils\\convert.py:256\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[1;34m(G, group_node_attrs, group_edge_attrs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (_, feat_dict) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(feat_dict\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(node_attrs):\n\u001b[1;32m--> 256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot all nodes contain the same attributes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m feat_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    258\u001b[0m         data_dict[\u001b[38;5;28mstr\u001b[39m(key)]\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "\u001b[1;31mValueError\u001b[0m: Not all nodes contain the same attributes"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, SAGEConv, global_mean_pool, BatchNorm, LayerNorm\n",
    "from torch_geometric.data import Data, Batch\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedTelomeraseGNN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels, num_heads, num_node_types):\n",
    "        super(EnhancedTelomeraseGNN, self).__init__()\n",
    "        self.node_embedding = nn.Embedding(num_node_types, hidden_channels)\n",
    "        self.edge_embedding = nn.Linear(num_edge_features, hidden_channels)\n",
    "        \n",
    "        self.conv1 = GATv2Conv(hidden_channels + num_node_features, hidden_channels, heads=num_heads, edge_dim=hidden_channels, concat=False)\n",
    "        self.norm1 = BatchNorm(hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels, heads=num_heads, edge_dim=hidden_channels, concat=False)\n",
    "        self.norm2 = BatchNorm(hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.norm3 = BatchNorm(hidden_channels)\n",
    "        self.layernorm1 = LayerNorm(hidden_channels)\n",
    "        \n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.telomerase_activity_head = nn.Linear(hidden_channels, 1)\n",
    "        self.compound_effectiveness_head = nn.Linear(hidden_channels, 1)\n",
    "        self.pchembl_value_head = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, node_type):\n",
    "        node_embed = self.node_embedding(node_type)\n",
    "        x = torch.cat([x, node_embed], dim=-1)\n",
    "        edge_embed = self.edge_embedding(edge_attr)\n",
    "\n",
    "        x = self.norm1(F.elu(self.conv1(x, edge_index, edge_attr=edge_embed)))\n",
    "        x = self.norm2(F.elu(self.conv2(x, edge_index, edge_attr=edge_embed)))\n",
    "        x = self.norm3(F.elu(self.conv3(x, edge_index)))\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.elu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.7, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        telomerase_activity = self.telomerase_activity_head(x)\n",
    "        compound_effectiveness = self.compound_effectiveness_head(x)\n",
    "        pchembl_value = self.pchembl_value_head(x)\n",
    "\n",
    "        return telomerase_activity, compound_effectiveness, pchembl_value\n",
    "    \n",
    "    \n",
    "def load_knowledge_graph(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "    \n",
    "    if not isinstance(G, nx.Graph):\n",
    "        raise TypeError(\"The loaded object is not a NetworkX graph\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(G):\n",
    "    data = from_networkx(G)\n",
    "    \n",
    "    # Create node features\n",
    "    node_features = []\n",
    "    for _, attrs in G.nodes(data=True):\n",
    "        features = [\n",
    "            attrs.get('MolWt', 0),\n",
    "            attrs.get('LogP', 0),\n",
    "            attrs.get('NumHDonors', 0),\n",
    "            attrs.get('NumHAcceptors', 0),\n",
    "            attrs.get('NumRotatableBonds', 0),\n",
    "            attrs.get('NumAromaticRings', 0)\n",
    "        ]\n",
    "        node_features.append(features)\n",
    "    data.x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge features\n",
    "    edge_features = []\n",
    "    for _, _, attrs in G.edges(data=True):\n",
    "        edge_features.append([attrs.get('activity_value', 0)])\n",
    "    data.edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    \n",
    "    # Create node types\n",
    "    node_types = [G.nodes[n].get('type', 'unknown') for n in G.nodes()]\n",
    "    le = LabelEncoder()\n",
    "    encoded_node_types = le.fit_transform(node_types)\n",
    "    data.node_type = torch.tensor(encoded_node_types, dtype=torch.long)\n",
    "    \n",
    "    # Store the label encoder for future use\n",
    "    data.type_encoder = le\n",
    "    \n",
    "    # Create target variables\n",
    "    data.telomerase_activity = torch.tensor([G.nodes[n].get('telomerase_activity', 0) for n in G.nodes()]).float().unsqueeze(1)\n",
    "    data.compound_effectiveness = torch.tensor([G.nodes[n].get('compound_effectiveness', 0) for n in G.nodes()]).float().unsqueeze(1)\n",
    "    data.pchembl_value = torch.tensor([G.nodes[n].get('pchembl_value', 0) for n in G.nodes()]).float().unsqueeze(1)\n",
    "    \n",
    "    logger.info(f\"Number of nodes: {data.num_nodes}\")\n",
    "    logger.info(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "    logger.info(f\"Edge attr shape: {data.edge_attr.shape}\")\n",
    "    logger.info(f\"Node feature shape: {data.x.shape}\")\n",
    "    logger.info(f\"Node type shape: {data.node_type.shape}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_model(model, data, device, num_epochs=100, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Split the data into train and validation sets\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[:int(0.8 * num_nodes)] = True\n",
    "    val_mask = ~train_mask\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch+1}: Input shapes:\")\n",
    "        logger.info(f\"x: {data.x.shape}\")\n",
    "        logger.info(f\"edge_index: {data.edge_index.shape}\")\n",
    "        logger.info(f\"edge_attr: {data.edge_attr.shape}\")\n",
    "        logger.info(f\"node_type: {data.node_type.shape}\")\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.edge_attr, None, data.node_type)\n",
    "        loss = (criterion(out[0][train_mask], data.telomerase_activity[train_mask]) + \n",
    "                criterion(out[1][train_mask], data.compound_effectiveness[train_mask]) + \n",
    "                criterion(out[2][train_mask], data.pchembl_value[train_mask]))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, None, data.node_type)\n",
    "            val_loss = (criterion(out[0][val_mask], data.telomerase_activity[val_mask]) + \n",
    "                        criterion(out[1][val_mask], data.compound_effectiveness[val_mask]) + \n",
    "                        criterion(out[2][val_mask], data.pchembl_value[val_mask]))\n",
    "        \n",
    "        logger.info(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "                'num_node_features': model.conv1.in_channels - model.node_embedding.embedding_dim,\n",
    "                'num_edge_features': model.edge_embedding.in_features,\n",
    "                'hidden_channels': model.conv1.out_channels,\n",
    "                'num_heads': model.conv1.heads,\n",
    "                'num_node_types': model.node_embedding.num_embeddings\n",
    "            }, 'best_model.pth')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the knowledge graph\n",
    "    graph_path = r\"C:\\Users\\wes\\telomerase_comprehensive_knowledge_graph_updated_5.pkl\"\n",
    "    G = load_knowledge_graph(graph_path)\n",
    "    \n",
    "    # Prepare the data\n",
    "    data = prepare_data(G)\n",
    "    data = data.to(device)\n",
    "    logger.info(f\"Data moved to device: {data.x.device}\")\n",
    "\n",
    "    num_node_features = data.x.size(1)\n",
    "    num_edge_features = data.edge_attr.size(1)\n",
    "    num_node_types = len(data.type_encoder.classes_)\n",
    "\n",
    "    model = EnhancedTelomeraseGNN(\n",
    "        num_node_features=num_node_features,\n",
    "        num_edge_features=num_edge_features,\n",
    "        hidden_channels=64,\n",
    "        num_heads=4,\n",
    "        num_node_types=num_node_types\n",
    "    ).to(device)\n",
    "\n",
    "    trained_model = train_model(model, data, device)\n",
    "    logger.info(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated valid SMILES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:15:30] SMILES Parse Error: syntax error while parsing: SL1(AT)YL2LL(-c3DE(C)o3)cALO)c(PS3KL(AV)c3)c2CALO)N1\n",
      "[19:15:30] SMILES Parse Error: Failed parsing SMILES 'SL1(AT)YL2LL(-c3DE(C)o3)cALO)c(PS3KL(AV)c3)c2CALO)N1' for input: 'SL1(AT)YL2LL(-c3DE(C)o3)cALO)c(PS3KL(AV)c3)c2CALO)N1'\n",
      "[19:15:30] SMILES Parse Error: syntax error while parsing: IT1KL(c1)N1CALO)c2LL(5NW5LL2C1=O)C1AA3(C)EE(NQLEE3\n",
      "[19:15:30] SMILES Parse Error: Failed parsing SMILES 'IT1KL(c1)N1CALO)c2LL(5NW5LL2C1=O)C1AA3(C)EE(NQLEE3' for input: 'IT1KL(c1)N1CALO)c2LL(5NW5LL2C1=O)C1AA3(C)EE(NQLEE3'\n",
      "[19:15:30] SMILES Parse Error: syntax error while parsing: AA[C@H]1N(VG2c1KL2O)GL1LL(C=O)VL1F\n",
      "[19:15:30] SMILES Parse Error: Failed parsing SMILES 'AA[C@H]1N(VG2c1KL2O)GL1LL(C=O)VL1F' for input: 'AA[C@H]1N(VG2c1KL2O)GL1LL(C=O)VL1F'\n",
      "[19:15:30] SMILES Parse Error: syntax error while parsing: ATALO)c1LL2[C@H]3O[C@DDCILOAEC@@HDDOAEC@HDDO)AA3(C)c2c(n1DPc1KL(c1)C(F)F\n",
      "[19:15:30] SMILES Parse Error: Failed parsing SMILES 'ATALO)c1LL2[C@H]3O[C@DDCILOAEC@@HDDOAEC@HDDO)AA3(C)c2c(n1DPc1KL(c1)C(F)F' for input: 'ATALO)c1LL2[C@H]3O[C@DDCILOAEC@@HDDOAEC@HDDO)AA3(C)c2c(n1DPc1KL(c1)C(F)F'\n",
      "[19:15:30] SMILES Parse Error: syntax error while parsing: PL1KL(c1DPn1DE2[C@HDDGL3GG3)N(VGVS)CALO)SL1VY1\n",
      "[19:15:30] SMILES Parse Error: Failed parsing SMILES 'PL1KL(c1DPn1DE2[C@HDDGL3GG3)N(VGVS)CALO)SL1VY1' for input: 'PL1KL(c1DPn1DE2[C@HDDGL3GG3)N(VGVS)CALO)SL1VY1'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rdkit import Chem\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol is not None\n",
    "\n",
    "# Load the model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gokceuludogan/WarmMolGenTwo\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gokceuludogan/WarmMolGenTwo\")\n",
    "def generate_ligand_smiles(protein_sequence, num_samples=5, max_length=512):\n",
    "    # Prepare the input\n",
    "    input_text = f\"<protein>{protein_sequence}</protein><smiles>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate SMILES\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_samples,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    # Decode and validate SMILES\n",
    "    generated_smiles = []\n",
    "    for output in outputs:\n",
    "        smiles = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        smiles = smiles.split(\"<smiles>\")[-1].strip()\n",
    "        if is_valid_smiles(smiles):\n",
    "            generated_smiles.append(smiles)\n",
    "\n",
    "    return generated_smiles\n",
    "\n",
    "# Example usage\n",
    "protein_sequence = \"ISLPKIPFIRYPIKVRQKAWPWHFKIWIYKSAIDHKLASMMGKWTPDQTKPNDTWCFESGPWVERLEKFILNQMITPLTGSENCYWMNHNVSKNTCYTGHLRTSKWEPQEHGIRDFFKFKDNKPFHWDGAAKMQDKRLDQQITRGKEPQNGRETQQMRPGKKILDVCQMSDKLNEPQPWEGDKSVGSQPQWRVHVYPEYHFHCKSHGADLIVPMHHPWCAQMLMGNWYVAPLKSWSFLLMMAQNDTPNWIYKEIMLYCSPRDSFHMNKNFYDSAHQNKENILNLCGNDCDGYKGESVKQCGPDAGVSHVMLPHSKYGKWPRTFAAGQHMWMQEFWINNNQKNENGCRCHYTNLEVSKHDEHQIHVMMGNCATIVTPHVMYLDWASQEMERHRENQYDYVYICSASRDRAGTTKIMWGNGNKALPNLGSKGNLECWGAGIQQVDILWKYKMQNRAPHSLIFGDQLSVWLECCIHNGQGPKQERLSAHEQSFCVFNMRNQVRKKSFHP\"\n",
    "ligand_smiles = generate_ligand_smiles(protein_sequence)\n",
    "\n",
    "print(\"Generated valid SMILES:\")\n",
    "for smiles in ligand_smiles:\n",
    "    print(smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity of generated SMILES:\n",
      "COc1cccc(c1)-c1cc(-c2ccccc2)n2nc(C)cc2n1: True\n",
      "CC(C)N1CC(CC(O)=O)n2cc(C#N)c(=O)cc2C1=O: True\n",
      "CCC1(CC)O[C@@]2(C)OC(NC)=C(C(O)O)[C@@H]2O1: True\n",
      "2C[C@@]1(NC(=O)c1csc(n1)-c1ccc(F)cc1F)C1CC1: False\n",
      "CCC[C@]1(O)CN(C2CCCC2)C(=O)c2c(O)c(=O)c(cn12)-c1csc(C)n1: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:20:45] SMILES Parse Error: syntax error while parsing: 2C[C@@]1(NC(=O)c1csc(n1)-c1ccc(F)cc1F)C1CC1\n",
      "[19:20:45] SMILES Parse Error: Failed parsing SMILES '2C[C@@]1(NC(=O)c1csc(n1)-c1ccc(F)cc1F)C1CC1' for input: '2C[C@@]1(NC(=O)c1csc(n1)-c1ccc(F)cc1F)C1CC1'\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, RobertaTokenizer, pipeline\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol is not None\n",
    "\n",
    "\n",
    "\n",
    "protein_tokenizer = RobertaTokenizer.from_pretrained(\"gokceuludogan/WarmMolGenTwo\")\n",
    "mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"gokceuludogan/WarmMolGenTwo\")\n",
    "inputs = protein_tokenizer(\"ISLPKIPFIRYPIKVRQKAWPWHFKIWIYKSAIDHKLASMMGKWTPDQTKPNDTWCFESGPWVERLEKFILNQMITPLTGSENCYWMNHNVSKNTCYTGHLRTSKWEPQEHGIRDFFKFKDNKPFHWDGAAKMQDKRLDQQITRGKEPQNGRETQQMRPGKKILDVCQMSDKLNEPQPWEGDKSVGSQPQWRVHVYPEYHFHCKSHGADLIVPMHHPWCAQMLMGNWYVAPLKSWSFLLMMAQNDTPNWIYKEIMLYCSPRDSFHMNKNFYDSAHQNKENILNLCGNDCDGYKGESVKQCGPDAGVSHVMLPHSKYGKWPRTFAAGQHMWMQEFWINNNQKNENGCRCHYTNLEVSKHDEHQIHVMMGNCATIVTPHVMYLDWASQEMERHRENQYDYVYICSASRDRAGTTKIMWGNGNKALPNLGSKGNLECWGAGIQQVDILWKYKMQNRAPHSLIFGDQLSVWLECCIHNGQGPKQERLSAHEQSFCVFNMRNQVRKKSFHP\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, decoder_start_token_id=mol_tokenizer.bos_token_id, \n",
    "                          eos_token_id=mol_tokenizer.eos_token_id, pad_token_id=mol_tokenizer.eos_token_id, \n",
    "                          max_length=256, num_return_sequences=5, do_sample=True, top_p=0.95)\n",
    "\n",
    "\n",
    "generated_smiles = mol_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "print(\"Validity of generated SMILES:\")\n",
    "for smiles in generated_smiles:\n",
    "    print(f\"{smiles}: {is_valid_smiles(smiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
